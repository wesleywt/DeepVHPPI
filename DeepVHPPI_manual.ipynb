{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Main notebook for training models using DeepVHPPI\n",
    "This notebook is the main notebook for doing experiments and coding in DeepVHPPI\n",
    "\n",
    "## Running terminal commands and bash in Jupyter\n",
    "You can run a terminal command by pre-pending the command with a ```!```.\n",
    "\n",
    "If need the terminal output to be printed then ```!``` will not work. You will need to pre-pend the ```%run``` magic command\n",
    "\n",
    "To run a bash script add %%bash at the top of the cell\n",
    "\n",
    "Here we can start the training for the Zhou virus-host interaction training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "%run -i main.py --data_root ./data/ -tr zhou/h1n1/human/train.json  -va zhou/h1n1/human/test.json -v vocab.data -s 1024 -hs 512 -l 12  -o results --lr 0.00001 --dropout 0.1 --epochs 20000 --attn_heads 8 --activation gelu --task ppi --emb_type conv --overwrite  --batch_size 2 --grad_ac_steps 2 --name ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets for TB training\n",
    "### HPIDB dataset\n",
    "This dataset was obtained from [HPIDB](https://hpidb.igbb.msstate.edu/) in 2022, a host-pathogen database containing experimental and predicted interactions between various hosts and pathogens. 10704 host-bacterial pathogen interactions were downloaded by clicking [The Pie Chart](https://hpidb.igbb.msstate.edu/hpi30_statistics.html), bacteria section.\n",
    "\n",
    "The HPIDB dataset serves as the training dataset for PPI prediction training using the BERT model.\n",
    "\n",
    "#### Negative HPIDB dataset\n",
    "The negative interaction dataset was created by downloading a random sequence from Uniprot and pairing it with a bacterial pathogen sequence. This created a negative human-pathogen interaction dataset that can be used for training. To ensure that no human sequence in the positive dataset occurred, we used CD-HIT-2D to compare the sequence similarity of the sequences in both datasets.\n",
    "\n",
    "1. A negative set of sequences of len == length of positive set of human sequences.\n",
    "2. CD-HIT finds examples in the negative dataset that is greater than 80% similarity in the positive dataset.\n",
    "3. We remove the examples and replace them with new examples (create a new list) and compare this list again to the positive dataset.\n",
    "4. Finally, we will end with a positive and negative dataset that can be used for training.\n",
    "\n",
    "## Training the MTB dataset\n",
    "I used the same parameters for training this dataset as that was used for Zhou PPIs. Here is the commandline instruction. The batch_size was reduced from 8 to 2, as I am training on my desktop RTX3060 and not on V100s. It will take weeks to get to 20000 epochs, but training can be interrupted and restarted by loading already trained model parameters.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run -i main.py --data_root ./data/ -tr williams_MTB/hpidb_train.json  -va williams_MTB/hpidb_test.json  -te williams_MTB/mt37_HPI_test.json  -v vocab.data -s 1024 -hs 512 -l 12  -o results --lr 0.00001 --dropout 0.1 --epochs 20000 --attn_heads 8 --activation gelu --task ppi --emb_type conv --overwrite  --batch_size 2 --grad_ac_steps 2 --name ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not there are 8384 iterations because we have a batch_size of 2.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading from pre-trained\n",
    "If we interrupt training we can start the training again by loading from already trained model parameters.\n",
    "\n",
    "Add the extra parameter ```--saved_model``` and point to the file best_model.pt to start from a checkpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run -i main.py --data_root ./data/ -tr williams_MTB/hpidb_train.json  -va williams_MTB/hpidb_test.json  -te williams_MTB/mt37_HPI_test.json  -v vocab.data -s 1024 -hs 512 -l 12  -o results --lr 0.00001 --dropout 0.1 --epochs 20000 --attn_heads 8 --activation gelu --task ppi --emb_type conv --overwrite  --batch_size 2 --grad_ac_steps 2 --name from_saved --saved_model results/ppi.bert.bsz_4.layers_12.size_512.heads_8.drop_10.lr_1e-05.emb_conv.saved_model.h1n1.'mtb2'/best_model.pt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viewing Training Progress\n",
    "Even though we can use useful tools like Weights and Bias, for now we will use Plotly to view the training progress locally."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import plotly.express as px\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I created two functions:\n",
    "\n",
    "1. To show the graph\n",
    "2. To load the json formatted log file from disk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_graph(df, metric, title):\n",
    "    fig = px.line(df, x='epoch', y=metric, title=title)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def log_to_pandas(path_to_log_file):\n",
    "    with open(path_to_log_file, 'r') as f:\n",
    "        log = json.load(f)\n",
    "    log_df = pd.DataFrame([log]).T\n",
    "    normalized = pd.json_normalize(log_df[0])\n",
    "    log_df = normalized.reset_index().rename({'index': 'epoch'}, axis='columns')\n",
    "    return log_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_log = log_to_pandas(\"results/ppi.bert.bsz_4.layers_12.size_512.heads_8.drop_10.lr_1e-05.emb_conv.saved_model.h1n1.'mtb2'/train_log.json\")\n",
    "show_graph(training_log, 'acc', 'Training Accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_graph(training_log, 'loss', 'Training Loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_log"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to view the Vocab data\n",
    "The data file that makes up the vocabulary of the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from data import WordVocab\n",
    "\n",
    "vocab = WordVocab.load_vocab('data/vocab.data')\n",
    "print(len(vocab))\n",
    "print(vocab.stoi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Viewing how nn.Embedding works\n",
    "We need to first tokenize the sentence and convert these tokens to tensors\n",
    "We can then use nn.Embedding to create a one-hot encoded vector of the input sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "onehot = nn.Embedding(29, 28, padding_idx=0)\n",
    "onehot.weight.requires_grad = False\n",
    "onehot.weight[1:] = torch.eye(28)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_seq(sentence, vocab):\n",
    "    tokens = list(sentence)\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = vocab.stoi.get(token, vocab.unk_index)\n",
    "\n",
    "    tokens = [vocab.cls_index] + tokens\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq = process_seq('MTAVVATA', vocab)\n",
    "seq = torch.tensor(seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onehot(seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = list('MTAVVATA')\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab.stoi.get('U', vocab.unk_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to view the Vocab data\n",
    "The data file that makes up the vocabulary of the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "{'<pad>': 0, '<mask>': 1, '<cls>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'I': 9, 'E': 10, 'R': 11, 'D': 12, 'T': 13, 'K': 14, 'P': 15, 'F': 16, 'N': 17, 'Q': 18, 'Y': 19, 'H': 20, 'M': 21, 'W': 22, 'C': 23, 'X': 24, 'U': 25, 'O': 26, 'Z': 27, 'B': 28}\n"
     ]
    }
   ],
   "source": [
    "from data import WordVocab\n",
    "\n",
    "vocab = WordVocab.load_vocab('data/vocab.data')\n",
    "print(len(vocab))\n",
    "print(vocab.stoi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Viewing how nn.Embedding works\n",
    "We need to first tokenize the sentence and convert these tokens to tensors\n",
    "We can then use nn.Embedding to create a one-hot encoded vector of the input sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "onehot = nn.Embedding(29, 28, padding_idx=0)\n",
    "onehot.weight.requires_grad = False\n",
    "onehot.weight[1:] = torch.eye(28)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def process_seq(sentence, vocab):\n",
    "    tokens = list(sentence)\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens[i] = vocab.stoi.get(token, vocab.unk_index)\n",
    "\n",
    "    tokens = [vocab.cls_index] + tokens\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "seq = process_seq('MTAVVATA', vocab)\n",
    "seq = torch.tensor(seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "['M', 'T', 'A', 'V', 'V', 'A', 'T', 'A']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list('MTAVVATA')\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi.get('U', vocab.unk_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}